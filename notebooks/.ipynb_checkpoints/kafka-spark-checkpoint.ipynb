{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f05be18b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sh\n",
    "pip install kafka-python psycopg2-binary elasticsearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ca07522",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import json\n",
    "import random\n",
    "import logging\n",
    "import requests\n",
    "from kafka import KafkaProducer\n",
    "from kafka.errors import KafkaError\n",
    "\n",
    "STATS_SYMBOLS = ['IBM', 'AAPL', 'MSFT', 'AMZN', 'FB', 'TSLA', 'BABA', 'TSM']\n",
    "KAFKA_BROKER = \"172.27.0.12:9092\"\n",
    "KAFKA_TOPIC = \"my_test3\"\n",
    "API_KEY = 'Z1OYPV3VSJIAWIBV'\n",
    "\n",
    "# logging.basicConfig(level=logging.DEBUG)\n",
    "producer = KafkaProducer(bootstrap_servers=[KAFKA_BROKER])\n",
    "\n",
    "for symbol in STATS_SYMBOLS:\n",
    "    r = requests.get('https://www.alphavantage.co/query?function=TIME_SERIES_DAILY_ADJUSTED&symbol={}&outputsize=full&apikey={}'.format(symbol, API_KEY))\n",
    "    data = r.json()['Time Series (Daily)']\n",
    "    \n",
    "    for rec in data:\n",
    "        data[rec]['date'] = rec\n",
    "        data[rec]['symbol'] = symbol\n",
    "        data[rec]['type'] = 'daily_adjusted_series'\n",
    "        \n",
    "        future = producer.send(KAFKA_TOPIC, json.dumps(data[rec]).encode('utf-8'))\n",
    "        \n",
    "        try:\n",
    "            record_metadata = future.get(timeout=10)\n",
    "        except KafkaError:\n",
    "            print('err')\n",
    "            pass\n",
    "        \n",
    "        producer.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33cb4f8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    from urllib.request import urlopen\n",
    "except ImportError:\n",
    "    from urllib2 import urlopen\n",
    "\n",
    "import json\n",
    "import time\n",
    "import json\n",
    "import random\n",
    "import logging\n",
    "import requests\n",
    "from kafka import KafkaProducer\n",
    "from kafka.errors import KafkaError\n",
    "\n",
    "KAFKA_BROKER = \"172.27.0.12:9092\"\n",
    "KAFKA_TOPIC = \"my_test4\"\n",
    "\n",
    "producer = KafkaProducer(bootstrap_servers=[KAFKA_BROKER])\n",
    "\n",
    "def get_jsonparsed_data(url):\n",
    "    response = urlopen(url)\n",
    "    data = response.read().decode(\"utf-8\")\n",
    "    return json.loads(data)\n",
    "\n",
    "url = (\"https://financialmodelingprep.com/api/v3/stock_news?tickers=AAPL,FB,GOOG,AMZN&limit=50&apikey=a3ff475f3e9cd554cb4cd256735c08cd\")\n",
    "data = get_jsonparsed_data(url)\n",
    "\n",
    "for rec in data:\n",
    "    rec['type'] = 'stock_news'\n",
    "    \n",
    "    future = producer.send(KAFKA_TOPIC, json.dumps(rec).encode('utf-8'))\n",
    "\n",
    "    try:\n",
    "        record_metadata = future.get(timeout=10)\n",
    "    except KafkaError:\n",
    "        print('err')\n",
    "        pass\n",
    "\n",
    "    producer.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b7b7468",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "from pyspark import SparkContext\n",
    "from pyspark.streaming import StreamingContext\n",
    "from pyspark.streaming.kafka import KafkaUtils\n",
    "from elasticsearch import Elasticsearch\n",
    "import psycopg2\n",
    "import json\n",
    "import os\n",
    "import time\n",
    "import requests\n",
    "\n",
    "os.environ['PYSPARK_SUBMIT_ARGS'] = '--packages org.apache.spark:spark-streaming-kafka-0-8_2.11:2.4.7 pyspark-shell'\n",
    "\n",
    "DAILY_SERIES_REKEY_MAP =  { # alpha vantage api\n",
    "    '1. open': 'open',\n",
    "    '2. high': 'high',\n",
    "    '3. low': 'low',\n",
    "    '4. close': 'close',\n",
    "    '5. adjusted close': 'adjusted_close',\n",
    "    '6. volume': 'volume',\n",
    "    '7. dividend amount': 'dividend_amount',\n",
    "    '8. split coefficient': 'split_coefficient'\n",
    "}\n",
    "\n",
    "DAILY_SERIES_PG_QUERY = \"\"\"\n",
    "            insert into daily_adjusted_series\n",
    "            (open, high, low, close, adjusted_close, volume, dividend_amount, split_coefficient, series_date, symbol)\n",
    "            values (%s, %s, %s, %s, %s, %s, %s, %s, %s, %s)\n",
    "            \"\"\"\n",
    "\n",
    "STOCK_NEWS_PG_QUERY = \"\"\"\n",
    "            insert into stock_news\n",
    "            (title, image, site, news_text, url, published_date, symbol)\n",
    "            values (%s, %s, %s, %s, %s, %s, %s)\n",
    "            \"\"\"\n",
    "\n",
    "def telegram_bot_send(msg):\n",
    "    bot_token = '1799138792:AAGzrwurIqPpJp0Vc6SJ1MuoZl8zMJ3JEWs'\n",
    "    bot_chat_id = '131231613'\n",
    "    send_text = 'https://api.telegram.org/bot' + bot_token + '/sendMessage?chat_id=' + bot_chat_id + '&parse_mode=Markdown&text=' + msg\n",
    "    response = requests.get(send_text)\n",
    "    return response.json()\n",
    "    \n",
    "class Pg:\n",
    "    def __init__(self):\n",
    "        self._conn = psycopg2.connect(user=\"stockanalytics\",\n",
    "                          password=\"stockanalytics\",\n",
    "                          host=\"172.27.0.55\",\n",
    "                          port=\"5432\",\n",
    "                          database=\"stockanalytics\")\n",
    "        \n",
    "    def execute(self, command, args = ()):\n",
    "        try:\n",
    "            self._cur = self._conn.cursor()\n",
    "\n",
    "            if args:\n",
    "                self._cur.execute(command, args)\n",
    "            else:\n",
    "                self._cur.execute(command)\n",
    "\n",
    "        except IndexError:\n",
    "            telegram_bot_send('Error.')\n",
    "            telegram_bot_send(','.join(args))\n",
    "            telegram_bot_send(command)\n",
    "            exit()\n",
    "                \n",
    "    def commit(self):\n",
    "        self._cur.close()\n",
    "        self._conn.commit()\n",
    "\n",
    "class Elastic:\n",
    "    def __init__(self):\n",
    "        self._es = Elasticsearch(['http://172.27.0.88:9200'], http_auth=('elastic', 'T4EoCBxfGZ4Ytm3GIzVu'))\n",
    "    \n",
    "    def insert_index(self, doc):\n",
    "        res = self._es.index(index=\"daily_series\", body=doc)\n",
    "        \n",
    "    def refresh(self):\n",
    "        self._es.indices.refresh(index=\"daily_series\")\n",
    "\n",
    "es = Elastic()\n",
    "pg = Pg()\n",
    "\n",
    "sc = SparkContext.getOrCreate()\n",
    "sc.setLogLevel(\"WARN\")\n",
    "\n",
    "ssc = StreamingContext(sc, 60)\n",
    "opts = {\"metadata.broker.list\":\"172.27.0.12:9092\", \"auto.offset.reset\": \"smallest\", \"group.id\": \"test_v1\"}\n",
    "kafkaStream = KafkaUtils.createDirectStream(ssc, ['my_test3', 'my_test4'], opts)\n",
    "parsed = kafkaStream.map(lambda v: json.loads(v[1]))\n",
    "\n",
    "pg.execute(\"delete from daily_adjusted_series\")\n",
    "\n",
    "def handler(rdd):\n",
    "    def rekey(inp_dict, keys_replace):\n",
    "        return {keys_replace.get(k, k): v for k, v in inp_dict.items()}\n",
    "    \n",
    "    if not rdd.isEmpty():\n",
    "        collection = rdd.collect()\n",
    "        \n",
    "        for el in collection:\n",
    "            if (el['type'] == 'daily_adjusted_series'):\n",
    "                source = rekey(el, DAILY_SERIES_REKEY_MAP)\n",
    "                \n",
    "                pg.execute(DAILY_SERIES_PG_QUERY, (\n",
    "                    source['open'],\n",
    "                    source['high'],\n",
    "                    source['low'],\n",
    "                    source['close'],\n",
    "                    source['adjusted_close'],\n",
    "                    source['volume'],\n",
    "                    source['dividend_amount'],\n",
    "                    source['split_coefficient'],\n",
    "                    source['date'],\n",
    "                    source['symbol']\n",
    "                ))\n",
    "\n",
    "                es.insert_index(doc = {\n",
    "                    'open': source['open'],\n",
    "                    'high': source['high'],\n",
    "                    'low': source['low'],\n",
    "                    'close': source['close'],\n",
    "                    'adjusted_close': source['adjusted_close'],\n",
    "                    'volume': source['volume'],\n",
    "                    'dividend_amount': source['dividend_amount'],\n",
    "                    'split_coefficient': source['split_coefficient'],\n",
    "                    'date': source['date'],\n",
    "                    'symbol': source['symbol']\n",
    "                })\n",
    "\n",
    "            else:\n",
    "                pg.execute(STOCK_NEWS_PG_QUERY, (\n",
    "                    el['title'],\n",
    "                    el['image'],\n",
    "                    el['site'],\n",
    "                    el['text'],\n",
    "                    el['url'],\n",
    "                    el['publishedDate'],\n",
    "                    el['symbol']\n",
    "                ))\n",
    "\n",
    "                es.insert_index(doc = {\n",
    "                    'title': el['title'],\n",
    "                    'image': el['image'],\n",
    "                    'site': el['site'],\n",
    "                    'news_text': el['text'],\n",
    "                    'url': el['url'],\n",
    "                    'published_date': el['publishedDate'],\n",
    "                    'symbol': el['symbol']\n",
    "                })\n",
    "              \n",
    "        pg.commit()\n",
    "        es.refresh()\n",
    "        print('done')\n",
    "        telegram_bot_send('Elastic and PostgreSQL were filled.')\n",
    "        \n",
    "parsed.foreachRDD(lambda x: handler(x))\n",
    "\n",
    "ssc.start()\n",
    "ssc.awaitTermination()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fdf0160",
   "metadata": {},
   "outputs": [],
   "source": [
    "import psycopg2\n",
    "\n",
    "\n",
    "def create_tables():\n",
    "    \"\"\" create tables in the PostgreSQL database\"\"\"\n",
    "    commands = [\n",
    "        \"\"\"\n",
    "        drop table if exists daily_adjusted_series \n",
    "        \"\"\",\n",
    "        \"\"\"\n",
    "        drop table if exists stock_news \n",
    "        \"\"\",\n",
    "        \"\"\"\n",
    "        create table daily_adjusted_series (\n",
    "            id serial primary key,\n",
    "            open decimal(12,2),\n",
    "            high decimal(12,2),\n",
    "            low decimal(12,2),\n",
    "            close decimal(12,2),\n",
    "            adjusted_close decimal(12,2),\n",
    "            volume decimal(12,2),\n",
    "            dividend_amount decimal(12,2),\n",
    "            split_coefficient decimal(12,2),\n",
    "            series_date date,\n",
    "            symbol varchar(10)\n",
    "        )\n",
    "        \"\"\",\n",
    "        \"\"\"\n",
    "        create table stock_news (\n",
    "            id serial primary key,\n",
    "            title text,\n",
    "            image text,\n",
    "            site text,\n",
    "            news_text text,\n",
    "            url text,\n",
    "            published_date date,\n",
    "            symbol varchar(10)\n",
    "        )\n",
    "        \"\"\"]\n",
    "    \n",
    "    conn = None\n",
    "    \n",
    "    try:\n",
    "        # connect to the PostgreSQL server\n",
    "        conn = psycopg2.connect(user=\"stockanalytics\",\n",
    "                                  password=\"stockanalytics\",\n",
    "                                  host=\"172.27.0.55\",\n",
    "                                  port=\"5432\",\n",
    "                                  database=\"stockanalytics\")\n",
    "        cur = conn.cursor()\n",
    "        # create table one by one\n",
    "        for command in commands:\n",
    "            cur.execute(command)\n",
    "        # close communication with the PostgreSQL database server\n",
    "        cur.close()\n",
    "        # commit the changes\n",
    "        conn.commit()\n",
    "    except (Exception, psycopg2.DatabaseError) as error:\n",
    "        print(error)\n",
    "    finally:\n",
    "        if conn is not None:\n",
    "            conn.close()\n",
    "            \n",
    "create_tables()\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ecdf470",
   "metadata": {},
   "outputs": [],
   "source": [
    "from elasticsearch import Elasticsearch\n",
    "import psycopg2\n",
    "import json\n",
    "import os\n",
    "import time\n",
    "import requests\n",
    "\n",
    "DAILY_SERIES_ELASTIC_MAPPING = {'properties': {\n",
    "    'open': {'type': 'double'},\n",
    "    'high': {'type': 'double'},\n",
    "    'low': {'type': 'double'},\n",
    "    'close': {'type': 'double'},\n",
    "    'adjusted_close': {'type': 'double'},\n",
    "    'volume': {'type': 'integer'},\n",
    "    'dividend_amount': {'type': 'double'},\n",
    "    'split_coefficient': {'type': 'double'},\n",
    "    'date': {'type': 'date'},\n",
    "    'symbol': {'type': 'text'}\n",
    "}}\n",
    "\n",
    "STOCK_NEWS_ELASTIC_MAPPING = {'properties': {\n",
    "    'symbol': {'type': 'text'},\n",
    "    'published_date': {'type': 'date'},\n",
    "    'title': {'type': 'text'},\n",
    "    'image': {'type': 'text'},\n",
    "    'site': {'type': 'text'},\n",
    "    'news_text': {'type': 'text'},\n",
    "    'url': {'type': 'text'}\n",
    "}}\n",
    "\n",
    "es = Elasticsearch(['http://172.27.0.88:9200'], http_auth=('elastic', 'T4EoCBxfGZ4Ytm3GIzVu'))\n",
    "\n",
    "es.indices.delete(index='daily_series')\n",
    "es.indices.create(index='daily_series', body={'mappings': DAILY_SERIES_ELASTIC_MAPPING})\n",
    "\n",
    "es.indices.delete(index='stock_news')\n",
    "es.indices.create(index='stock_news', body={'mappings': STOCK_NEWS_ELASTIC_MAPPING})\n",
    "\n",
    "print('done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b45198c8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
